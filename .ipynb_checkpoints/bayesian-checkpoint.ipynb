{
 "cells": [
  {
   "cell_type": "raw",
   "id": "243ff847",
   "metadata": {},
   "source": [
    "---\n",
    "title: Bayesian Inference\n",
    "execute:\n",
    "  echo: false\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c313929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pymc as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe41194",
   "metadata": {},
   "source": [
    "You can add options to executable code like this\n",
    "\n",
    "\n",
    "```{r}\n",
    "#| echo: false\n",
    "2 * 2\n",
    "```\n",
    "\n",
    "\n",
    "The `echo: false` option disables the printing of code (only output is displayed).\n",
    "\n",
    "\n",
    "# Bayesian Inference\n",
    "\n",
    "## Statistical Inference\n",
    "\n",
    "Statistical inference is the process in which we make inferences (estimates) about a population or problem space when we only have a sub-sample of the total probable outcomes. In the vast majority of cases, we can never collect data for the entire population of the problem space but instead can only sample a subset.\n",
    "\n",
    "For example, some classic problems would be estimating the average height of everyone on earth when we only have the heights of 200 people or determining what is the probability of loan defaults for all potential loan customers given that I have 200 customers and 2 have defaulted in the last year.\n",
    "\n",
    "There are broadly two main approaches to making estimates about a population given data from a sub-sample from the population, a frequentist approach and a Bayesian approach.\n",
    "\n",
    "## Frequestist Approach\n",
    "\n",
    "First lets define frequentist statistics. A frequentist approach to statistics would be concerned about calculating the long-run probability of an outcome or event. This is the method that you were taught in school involving a lot of math, probabilities and p-values. The implicit assumption that we're making with this approach is that we have some ability to determine the long-run probability of an outcome. For example the long-run probability of getting heads on a coin toss would be 0.5 based on what we know about the mechanics of a coin toss. However, what if thought that the coin might not be a \"fair\" coin? How would we arrive at the probability of heads then if we had data on tosses of that coin?\n",
    "\n",
    "A frequentist approach to solving this problem (also called frequentist inference) would be to create a hypothesis and test it.\n",
    "\n",
    "https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjRpbe58qH3AhWCM30KHaWkDREQFnoECBsQAw&url=https%3A%2F%2Fvault.hanover.edu%2F~altermattw%2Fcourses%2F220%2Freadings%2FStatistical_Inference.pdf&usg=AOvVaw1htPlxgll-9LxipNreRcQQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef44db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.6 # this is unknown\n",
    "ntoss = 100\n",
    "nsamples = 10000\n",
    "\n",
    "rng = np.random.RandomState(13) #15 for 40\n",
    "data = rng.binomial(n=1, size=ntoss, p=p)\n",
    "heads = data.sum()\n",
    "print(\"Number of heads in sample: \", heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4c0951",
   "metadata": {},
   "source": [
    "h0: p = 0.5 # the null hypothesis that the coin is fair...\n",
    "\n",
    "h1: p > 0.5 # the alternative hypothesis \n",
    "\n",
    "The goal here is to accept or reject the null hypothesis by determining the odds of getting your result under th null hypothesis that the coin is fair.\n",
    "\n",
    "Let's assume that the total population of coin flips ever is 10,000,000. then what is the probabality of getting the result above from that total population?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d2caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = np.random.binomial(n=1, size=10000000, p=0.5)\n",
    "\n",
    "print(f\"Flip {ntoss} coins, {nsamples} times...\")\n",
    "sample = np.random.choice(pop, size=(ntoss,nsamples))\n",
    "\n",
    "cnt = sample.sum(axis=0)\n",
    "cnt[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4c6c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"probability of getting {heads} heads or more: \", round(len(cnt[cnt >= heads])/nsamples*100,4) , \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa95a97",
   "metadata": {},
   "source": [
    "Then, because the odds of getting 61 heads on a fair coin is only 1.99%, I would reject the null hypothesis h0, that the coin is fair. This is your p-value, the odds of getting this result under the null hypothesis.\n",
    "\n",
    "## Issue #1: Just because the result is unlikely, doesn't make it impossible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2892d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(12) #15 for 40\n",
    "data = rng.binomial(n=1, size=ntoss, p=p)\n",
    "heads = data.sum()\n",
    "print(\"Number of heads in sample: \", heads)\n",
    "\n",
    "print(f\"probability of getting {heads} heads or more: \", round(len(cnt[cnt >= heads])/nsamples*100,4) , \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc42e1",
   "metadata": {},
   "source": [
    "Even though we generated this sample using the same probability of heads of 0.6 as before, now we can't reject the null hypothesis!\n",
    "\n",
    "## Issue #2: Picking the right hypothesises is hard\n",
    "\n",
    "In the example above, just because we've rejected the null hypothesis, it doesn't make the alternative hypothesis automatically correct! For example p < 0.5. While this is a simple example with only 3 possible outcomes for p, there are many real life examples where there are many possible explanations aside from the null hypothesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dd8f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.4 # this is unknown\n",
    "\n",
    "rng = np.random.RandomState(101) #15 for 40\n",
    "data = rng.binomial(n=1, size=ntoss, p=p)\n",
    "heads = data.sum()\n",
    "print(\"Number of heads in sample: \", heads)\n",
    "\n",
    "print(f\"probability of getting {heads} heads or more: \", round(len(cnt[cnt >= heads])/nsamples*100,4) , \"%\")\n",
    "\n",
    "plt.hist(sample.mean(axis=0));\n",
    "print(\n",
    "    \"2.5th Percentile = \" + str(np.percentile(sample.sum(axis=0), 2.5)) + \" heads\", \n",
    "    \"97.5th Percentile = \" + str(np.percentile(sample.sum(axis=0), 97.5)) + \" heads\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f45cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"probability of getting {heads} heads or more: \", round(len(cnt[cnt >= heads])/nsamples*100,4) , \"%\")\n",
    "print(f\"probability of getting {heads} heads or more: \", round(len(cnt[cnt <= heads])/nsamples*100,4) , \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d5d5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"probability of getting 5 heads or more: \", round(len(cnt[cnt >= 5])/nsamples*100,2) , \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1713ec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntoss = 100\n",
    "nsamples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470669d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rng.binomial(n=1, size=ntoss, p=p)\n",
    "heads = data.sum()\n",
    "print(\"Number of heads in sample: \", heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ec77d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.random.choice(pop, size=(ntoss,nsamples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0882bb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = sample.sum(axis=0)\n",
    "cnt[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc548ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"probability of getting {heads} heads or more: \", round(len(cnt[cnt >= heads])/nsamples*100,2) , \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42e3acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908178de",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.binom.cdf(k=1, n=10, p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd1ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae93a1fa",
   "metadata": {},
   "source": [
    "## Issues\n",
    "\n",
    "1. It does not tell you what the most likely answer is...\n",
    "1. It does not tell you if p=0.5 is wrong, just unlikely. You can still get 9 heads with p=0.5. This is your false positive rate.\n",
    "\n",
    "Note that what we've done above is not the true way to calculate the p-value as most statistical methods use formulas to calculate it vs simulations.\n",
    "\n",
    "\n",
    "## Bayesian Approach\n",
    "\n",
    "![https://xkcd.com/1132/](https://imgs.xkcd.com/comics/frequentists_vs_bayesians_2x.png)\n",
    "\n",
    "\n",
    "https://www.sciencedirect.com/topics/neuroscience/statistical-inference\n",
    "\n",
    "https://en.wikipedia.org/wiki/Frequentist_inference\n",
    "\n",
    "https://www.redjournal.org/article/S0360-3016(21)03256-9/fulltext\n",
    "\n",
    "https://corporatefinanceinstitute.com/resources/knowledge/other/hypothesis-testing/\n",
    "\n",
    "http://sellsidehandbook.com/2018/12/09/statistical-inference-and-hypothesis-testing/\n",
    "\n",
    "https://www.statisticshowto.com/frequentist-statistics/\n",
    "\n",
    "\n",
    "Bayesian inference and Bayesian statistics in general is named after the statistician Thomas Bayes. \n",
    "\n",
    "In contrast to the frequentist method, Bayesian inference is focused on the probability that something is true. It begins with a measure of belief in a particular model or number, then with the addition of data, this belief is updated to reflect this new data. In my opinion, it is a more intuitive and natural method for incorporating the scientific method into the analytical process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d7bcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with defining H and \n",
    "\n",
    "# H = 1, T = 0\n",
    "\n",
    "rng = np.random.RandomState(13)\n",
    "data = rng.binomial(n=1, size=10, p=0.5)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ae96bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26011c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in range(1, 10000):\n",
    "  a.append(rng.binomial(n=1, size=i, p=0.5).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.Series(a)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(20,5))\n",
    "ax.plot(a)\n",
    "ax.set_xlabel('Sample Size')\n",
    "ax.set_ylabel('Estimation of P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7b4208",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(20,5))\n",
    "ax.plot(a.index[::10], a[::10])\n",
    "ax.set_xlabel('Sample Size (every tenth)')\n",
    "ax.set_ylabel('Estimation of P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce7c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(2000000)\n",
    "coin = rng.binomial(1,0.5,10)\n",
    "\n",
    "coin\n",
    "\n",
    "H = np.arange(0,1.1,0.1)\n",
    "\n",
    "T = np.flip(H)\n",
    "\n",
    "P = np.array([1.]*len(H))\n",
    "\n",
    "P *= T\n",
    "plt.plot(P)\n",
    "\n",
    "for c in coin:\n",
    "    if c == 0:\n",
    "        P *= T\n",
    "    else:\n",
    "        P*= H"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
