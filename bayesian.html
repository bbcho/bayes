<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Bayesian Inference</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="bayesian_files/libs/clipboard/clipboard.min.js"></script>
<script src="bayesian_files/libs/quarto-html/quarto.js"></script>
<script src="bayesian_files/libs/quarto-html/popper.min.js"></script>
<script src="bayesian_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="bayesian_files/libs/quarto-html/anchor.min.js"></script>
<link href="bayesian_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="bayesian_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="bayesian_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="bayesian_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="bayesian_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="bayesian_files/libs/quarto-diagram/mermaid.min.js"></script>
<script src="bayesian_files/libs/quarto-diagram/mermaid-init.js"></script>
<link href="bayesian_files/libs/quarto-diagram/mermaid.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Bayesian Inference</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pymc <span class="im">as</span> pm</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> config</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You can add options to executable code like this</p>
<pre class="{r}"><code>#| echo: false
2 * 2</code></pre>
<p>The <code>echo: false</code> option disables the printing of code (only output is displayed).</p>
<section id="statistical-inference" class="level2">
<h2 class="anchored" data-anchor-id="statistical-inference">Statistical Inference</h2>
<p>Statistical inference is the process in which we make inferences (estimates) about a population or problem space when we only have a sub-sample of the total probable outcomes. In the vast majority of cases, we can never collect data for the entire population of the problem space but instead can only sample a subset.</p>
<p>For example, some classic problems would be estimating the average height of everyone on earth when we only have the heights of 200 people or determining what is the probability of loan defaults for all potential loan customers given that I have 200 customers and 2 have defaulted in the last year. Another example closer to home would be using the daily returns of a stock or futures contract form the last 2 years (our sample) to estimate the population of ALL possible daily returns - most likely used to model the returns of the stock on contract going forward in time.</p>
<p>There are broadly two main approaches to making estimates about a population given data from a sub-sample from the population, a Frequentist approach and a Bayesian approach.</p>
</section>
<section id="frequestist-approach" class="level2">
<h2 class="anchored" data-anchor-id="frequestist-approach">Frequestist Approach</h2>
<p>First lets define frequentist statistics. A frequentist approach to statistics is concerned with calculating the long-run probability of an outcome or event. This is the method that you were taught in school involving a lot of math, probabilities and p-values. The implicit assumption that we’re making with this approach is that we have some ability to determine the long-run probability of an outcome. For example the long-run probability of getting heads on a coin toss would be 0.5 based on what we know about the mechanics of a coin toss. However, what if thought that the coin might not be a “fair” coin? How would we arrive at the probability of heads then if we had data on tosses of that coin?</p>
<p>A Frequentist approach to solving this problem (also called frequentist inference) would be to create a hypothesis and test it. To do so, we’re going to first generate a sequence of 100 coin tosses using an unfair coin. <!-- https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjRpbe58qH3AhWCM30KHaWkDREQFnoECBsQAw&url=https%3A%2F%2Fvault.hanover.edu%2F~altermattw%2Fcourses%2F220%2Freadings%2FStatistical_Inference.pdf&usg=AOvVaw1htPlxgll-9LxipNreRcQQ --></p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> config.p2</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>ntoss <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.RandomState(<span class="dv">13</span>) <span class="co">#15 for 40</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> rng.binomial(n<span class="op">=</span><span class="dv">1</span>, size<span class="op">=</span>ntoss, p<span class="op">=</span>p)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>heads <span class="op">=</span> data.<span class="bu">sum</span>()</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of heads in sample: "</span>, heads)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of heads in sample:  61</code></pre>
</div>
</div>
<p>Next we have to create a null hypothesis which in our case is that the coin is a fair coin toss. The alternative hypothesis is that the probablity of heads is greater than 50% and thus unfair.</p>
<p>h0: p = 0.5 # the null hypothesis that the coin is fair…</p>
<p>h1: p &gt; 0.5 # the alternative hypothesis</p>
<p>The goal here is to accept or reject the null hypothesis by determining the odds of getting your result under the null hypothesis that the coin is fair.</p>
<p>Said another way, Frequentist inference asks the question, what would be the likelihood of getting 61 heads on 100 coin tosses if the coin was fair? To do this, we’ll do the 100 tosses of the fair coin, 10,000 times to see how often we get 61 or more heads out of a hundred.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>nsamples <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Flip </span><span class="sc">{</span>ntoss<span class="sc">}</span><span class="ss"> coins, </span><span class="sc">{</span>nsamples<span class="sc">}</span><span class="ss"> times...</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>pop <span class="op">=</span> rng.binomial(n<span class="op">=</span><span class="dv">1</span>, size<span class="op">=</span>ntoss<span class="op">*</span>nsamples, p<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>sample <span class="op">=</span> rng.choice(pop, size<span class="op">=</span>(ntoss,nsamples))</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>cnt <span class="op">=</span> sample.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of heads for each sample of 100 tosses (first 20 samples):</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(cnt[:<span class="dv">20</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Flip 100 coins, 10000 times...

Number of heads for each sample of 100 tosses (first 20 samples):

[46 46 54 59 52 45 54 55 58 47 64 44 46 56 47 54 50 52 48 50]</code></pre>
</div>
</div>
<p>Now let’s determine how many times we got 61 heads or more…</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"probability of getting </span><span class="sc">{</span>heads<span class="sc">}</span><span class="ss"> heads or more: "</span>, </span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  <span class="bu">round</span>(<span class="bu">len</span>(cnt[cnt <span class="op">&gt;=</span> heads])<span class="op">/</span>nsamples<span class="op">*</span><span class="dv">100</span>,<span class="dv">4</span>), <span class="st">"%"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>probability of getting 61 heads or more:  1.83 %</code></pre>
</div>
</div>
<p>Then, because the odds of getting 61 heads or more on a fair coin is only 1.83%, I would reject the null hypothesis h0 that the coin is fair. This is your p-value, the odds of getting this result under the null hypothesis. Note that most statisticians reject the null hypothesis if the p-value is less than 5%. We also call the p-value you false-positive rate - if the coin was fair it would only show 61 heads (and that the coin was unfair) 1.83% of the time, causing you to reject the null hypothesis incorrectly.</p>
<p>However, there are a number of issues with using this approach.</p>
</section>
<section id="issue-1-just-because-the-result-is-unlikely-doesnt-make-it-impossible" class="level2">
<h2 class="anchored" data-anchor-id="issue-1-just-because-the-result-is-unlikely-doesnt-make-it-impossible">Issue #1: Just because the result is unlikely, doesn’t make it impossible</h2>
<p>Remember, even with the fair coin, we still got 61 or more heads 1.83% of the time. Also, even an unfair coin can sometimes produce results that would be indicative of a fair coin…</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.RandomState(<span class="dv">12</span>) <span class="co"># 15 for 40</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> rng.binomial(n<span class="op">=</span><span class="dv">1</span>, size<span class="op">=</span>ntoss, p<span class="op">=</span>p)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>heads <span class="op">=</span> data.<span class="bu">sum</span>()</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of heads in sample: "</span>, heads)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"probability of getting </span><span class="sc">{</span>heads<span class="sc">}</span><span class="ss"> heads or more: "</span>, <span class="bu">round</span>(<span class="bu">len</span>(cnt[cnt <span class="op">&gt;=</span> heads])<span class="op">/</span>nsamples<span class="op">*</span><span class="dv">100</span>,<span class="dv">4</span>) , <span class="st">"%"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of heads in sample:  57
probability of getting 57 heads or more:  10.08 %</code></pre>
</div>
</div>
<p>Even though we generated this sample using the same probability of heads as before, now we can’t reject the null hypothesis!</p>
</section>
<section id="issue-2-picking-the-right-hypothesises-is-hard" class="level2">
<h2 class="anchored" data-anchor-id="issue-2-picking-the-right-hypothesises-is-hard">Issue #2: Picking the right hypothesises is hard</h2>
<p>In the example above, just because we’ve rejected the null hypothesis, it doesn’t actually tell us what the true value of <code>p</code> is! For example another alternative hypothesis could be <code>p &lt; 0.5</code>. While this is a simple example with only 3 possible outcomes for p, there are many real life examples where there are many possible explanations aside from the null hypothesis.</p>
<p>h0: p = 0.5</p>
<p>h1: p &gt; 0.5</p>
<p>h2: p &lt; 0.5</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> config.p3</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.RandomState(<span class="dv">101</span>) <span class="co">#15 for 40</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> rng.binomial(n<span class="op">=</span><span class="dv">1</span>, size<span class="op">=</span>ntoss, p<span class="op">=</span>p)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>heads <span class="op">=</span> data.<span class="bu">sum</span>()</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of heads in sample: "</span>, heads)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"probability of getting </span><span class="sc">{</span>heads<span class="sc">}</span><span class="ss"> heads or more: "</span>, <span class="bu">round</span>(<span class="bu">len</span>(cnt[cnt <span class="op">&gt;=</span> heads])<span class="op">/</span>nsamples<span class="op">*</span><span class="dv">100</span>,<span class="dv">4</span>) , <span class="st">"%"</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>plt.hist(sample.mean(axis<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"2.5th Percentile = "</span> <span class="op">+</span> <span class="bu">str</span>(np.percentile(sample.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>), <span class="fl">2.5</span>)) <span class="op">+</span> <span class="st">" heads"</span>, </span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"97.5th Percentile = "</span> <span class="op">+</span> <span class="bu">str</span>(np.percentile(sample.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>), <span class="fl">97.5</span>)) <span class="op">+</span> <span class="st">" heads"</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of heads in sample:  42
probability of getting 42 heads or more:  95.39 %
2.5th Percentile = 40.0 heads 97.5th Percentile = 60.0 heads</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="bayesian_files/figure-html/cell-7-output-2.png" width="583" height="411"></p>
</div>
</div>
<p>In this case, without knowing the answer which alternative hypothesis would you use? Also it’s bad practice just keep on testing various hypothesizes. This is because even with a result that shows a p-value of &lt; 5%, this still means that you could randomly get a false-positive. The chance of getting a false-positive with a p-value of 5% on 20 tests is 100%! i.e.&nbsp;random chance alone will give you a false-positive if you just run enough tests…</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.RandomState(<span class="dv">10</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="dv">20</span>):</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> rng.binomial(n<span class="op">=</span><span class="dv">1</span>, size<span class="op">=</span>ntoss, p<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    heads <span class="op">=</span> data.<span class="bu">sum</span>()</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"probability of getting </span><span class="sc">{</span>heads<span class="sc">}</span><span class="ss"> heads or more: "</span>, <span class="bu">round</span>(<span class="bu">len</span>(cnt[cnt <span class="op">&gt;=</span> heads])<span class="op">/</span>nsamples<span class="op">*</span><span class="dv">100</span>,<span class="dv">4</span>) , <span class="st">"%"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>probability of getting 49 heads or more:  62.57 %
probability of getting 49 heads or more:  62.57 %
probability of getting 51 heads or more:  46.45 %
probability of getting 52 heads or more:  38.84 %
probability of getting 58 heads or more:  6.91 %
probability of getting 44 heads or more:  90.3 %
probability of getting 47 heads or more:  75.92 %
probability of getting 57 heads or more:  10.08 %
probability of getting 48 heads or more:  69.7 %
probability of getting 42 heads or more:  95.39 %
probability of getting 46 heads or more:  81.77 %
probability of getting 49 heads or more:  62.57 %
probability of getting 50 heads or more:  54.58 %
probability of getting 48 heads or more:  69.7 %
probability of getting 52 heads or more:  38.84 %
probability of getting 57 heads or more:  10.08 %
probability of getting 51 heads or more:  46.45 %
probability of getting 45 heads or more:  86.56 %
probability of getting 61 heads or more:  1.83 %
probability of getting 47 heads or more:  75.92 %</code></pre>
</div>
</div>
<p>As you can see, even with a fair coin, we generated 61 heads. If we had this result, we would have rejected the null hypothesis incorrectly.</p>
</section>
<section id="trying-to-determine-the-long-run-probability-does-not-always-make-sense" class="level2">
<h2 class="anchored" data-anchor-id="trying-to-determine-the-long-run-probability-does-not-always-make-sense">Trying to Determine the Long-run Probability Does Not Always Make Sense</h2>
<p>In many classes of problems, the concept of a long-run probability doesn’t always make sense. For example what is the long-run probability that our Sun will go nova tomorrow? Who knows? We only have one sun and it’s never gone nova before!</p>
</section>
<section id="issues" class="level2">
<h2 class="anchored" data-anchor-id="issues">Issues</h2>
<p>So in summary, some issues with using a frequentist approach is that:</p>
<ol type="1">
<li>It does not tell you what the most likely answer is…</li>
<li>It does not tell you if p=0.5 is wrong, just unlikely. You can still get 90 out of a hundred heads with p=0.5. This is your false positive rate.</li>
<li>Long-run probabilities don’t always exist.</li>
</ol>
<p>Note that what we’ve done above is not the true way to calculate the p-value as most statistical methods use formulas to calculate it vs simulations.</p>
<p>That’s not to say a frequentist approach is bad and in fact bayesian inference converges to the frequentist approach with sufficiently large samples. However let’s now have a look at Bayesian Inference.</p>
</section>
<section id="bayesian-approach" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-approach">Bayesian Approach</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://imgs.xkcd.com/comics/frequentists_vs_bayesians_2x.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">https://xkcd.com/1132/</figcaption><p></p>
</figure>
</div>
<!-- https://www.sciencedirect.com/topics/neuroscience/statistical-inference -->
<!---->
<!-- https://en.wikipedia.org/wiki/Frequentist_inference -->
<!---->
<!-- https://www.redjournal.org/article/S0360-3016(21)03256-9/fulltext -->
<!---->
<!-- https://corporatefinanceinstitute.com/resources/knowledge/other/hypothesis-testing/ -->
<!---->
<!-- http://sellsidehandbook.com/2018/12/09/statistical-inference-and-hypothesis-testing/ -->
<!---->
<!-- https://www.statisticshowto.com/frequentist-statistics/ -->
<!---->
<!-- https://www.statisticshowto.com/bayesian-statistics/ -->
<!---->
<!-- https://stats.stackexchange.com/questions/2272/whats-the-difference-between-a-confidence-interval-and-a-credible-interval -->
<!---->
<!-- https://www.amazon.ca/Introduction-Bayesian-Statistics-William-Bolstad/dp/1118091566 -->
<p>Bayesian inference and Bayesian statistics in general is named after the statistician Thomas Bayes.</p>
<p>In contrast to the frequentist method, Bayesian inference is focused on the probability that something is true. It asks the question, “what do I think is the underlying data generating process” and then uses this to determine the probability of the data given the model.</p>
<p>It begins with a measure of belief in a particular model or number, then with the addition of data, this belief is updated to reflect this new data. In my opinion, it is a more intuitive and natural method for incorporating data into the analytical process.</p>
<p>The general recipe for Bayesian inference is:</p>
<ol type="1">
<li>Pick a model that could have generated the data that you see (i.e.&nbsp;linear model, binomial distribution etc…)</li>
<li>Pick a prior distribution for the parameters of the model based on your knowledge of the process if any (this can also be uninformed).</li>
<li>Count the number of ways that your assumed model could have generated your data for each value of your parameter(s).</li>
<li>Parameters with more ways to produce your data are more plausible - put this together in a posterior distribution for your parameters.</li>
</ol>
<p>Let’s start with the same example as in the frequentist example, let’s say we have a coin and we want to determine if it’s fair. Let’s generate a sequence of 10 coin tosses.</p>
<div class="cell" data-execution_count="8">
<div class="cell-output cell-output-stdout">
<pre><code>[0 1 0 0 0 1 0 0 0 0]</code></pre>
</div>
</div>
<p>Since this is a coin toss, we will pick a binomial distribution as the likely process that generated our data. That means we need to determine the value for <code>p</code>. Remember, <code>p=0.5</code> means it is a fair coin.</p>
<p>Based on the sequence above, what is the most probable value of <code>p</code>? A bayesian approach would ask how often would this sequence of data occur if <code>p</code> was 0.5? If it was 0.6? If it was 0.7? etc. We can then use this information to determine the most likely value of <code>p</code>.</p>
<p>Since we don’t have any information aside from the data on the value of <code>p</code> we will start with an uninformed prior (i.e.&nbsp;we will try all values of <code>p</code>). We will try a range of values for <code>p</code> from 0.1 to 0.9 and flip the coin 10 times to generate a sample. We will then repeat this sample of 10 flips 1000 times to see how often we see 2 heads for each value of <code>p</code>.</p>
<p>First we will run with <code>p=0.1</code>.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.RandomState(<span class="dv">13</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>ps <span class="op">=</span> [<span class="fl">0.1</span>,<span class="fl">0.2</span>,<span class="fl">0.3</span>,<span class="fl">0.4</span>,<span class="fl">0.5</span>,<span class="fl">0.6</span>,<span class="fl">0.7</span>,<span class="fl">0.8</span>,<span class="fl">0.9</span>]</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>cnt <span class="op">=</span> np.array([<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>])</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="dv">1000</span>):</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>  tmp <span class="op">=</span> rng.binomial(n<span class="op">=</span><span class="dv">1</span>, size<span class="op">=</span><span class="dv">10</span>, p<span class="op">=</span>p)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> tmp.<span class="bu">sum</span>() <span class="op">==</span> data.<span class="bu">sum</span>():</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    cnt[<span class="dv">0</span>] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>ax.bar(x<span class="op">=</span>ps, height<span class="op">=</span>cnt, width<span class="op">=</span><span class="fl">0.1</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="bayesian_files/figure-html/cell-10-output-1.png" width="575" height="411"></p>
</div>
</div>
<p>Now let’s run it again for <code>p=0.2</code> so see how often we get 2 heads.</p>
<div class="cell" data-execution_count="10">
<div class="cell-output cell-output-display">
<p><img src="bayesian_files/figure-html/cell-11-output-1.png" width="575" height="411"></p>
</div>
</div>
<p>Then we will run it for all values for <code>p</code> between 0.1 and 0.9.</p>
<div class="cell" data-execution_count="11">
<div class="cell-output cell-output-display">
<p><img src="bayesian_files/figure-html/cell-12-output-1.png" width="575" height="411"></p>
</div>
</div>
<p>Finally we can normalize the data to get proper probablities for <code>p</code>.</p>
<div class="cell" data-execution_count="12">
<div class="cell-output cell-output-display">
<p><img src="bayesian_files/figure-html/cell-13-output-1.png" width="579" height="411"></p>
</div>
</div>
<p>This now gives us the number of times that 1000 draws of 10 coin tosses would give us 2 heads for each value of <code>p</code>. In other words we now have a probabiility distribution for the value of <code>p</code> given the data that we have (i.e.&nbsp;2 heads out of 10 coin tosses). This is the essence of bayesian inference, basically counting, where we make some assumption on the underlying data generating process and given this <em>a priori</em> assumption, we determine how likely each parameter is in generating the data that we see.</p>
<p>To repeat, our thought process was:</p>
<ol type="1">
<li>Because this is a coin toss, I think the underlying data generating process is a binomial distribution.</li>
<li>However, I do not know what the value of <code>p</code> is so I’m going to try all values of <code>p</code> (uninformed prior).</li>
<li>For each value of <code>p</code>, let’s count all the ways that my process can generate the data that I see.</li>
<li>Put this together into a posterior distribution for <code>p</code>. Values of <code>p</code> with more ways to produce my data are more plausible. This is my estimate for <code>p</code> with my uncertainty built in.</li>
</ol>
<p>I’m now going to introduce you to a slightly easier way for us to perform that same calculation. The issue with the above is that it is computationally expensive and hard to update with new data without rerunning the whole thing.</p>
<p>So similar to above, we’re still going to assume that the data generating process is a binomial distribution with parameter <code>p</code>. Also just like before, let’s assume that we have no information on <code>p</code> prior to drawing our sample of 10 coin tosses. So our prior estimate is a uniform distribution between 0 and 1 (i.e.&nbsp;any value of <code>p</code> is equally likely).</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>dist <span class="op">=</span> stats.beta</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> dist.pdf(x, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">4</span>,<span class="dv">2</span>))</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>plt.plot(x,y, label<span class="op">=</span><span class="st">'Initial Estimate for p'</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>plt.fill_between(x, <span class="dv">0</span>, y, color<span class="op">=</span><span class="st">"#348ABD"</span>, alpha<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>plt.vlines(<span class="fl">0.5</span>, <span class="dv">0</span>, <span class="dv">4</span>, color<span class="op">=</span><span class="st">"k"</span>, linestyles<span class="op">=</span><span class="st">"--"</span>, lw<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>plt.ylim((<span class="dv">0</span>,<span class="dv">3</span>))</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Prior distribution for p"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="bayesian_files/figure-html/cell-14-output-1.png" width="335" height="209"></p>
</div>
</div>
<p>Now let’s also define a probability density of Heads and Tails:</p>
<div class="cell" data-execution_count="14">
<div class="cell-output cell-output-display">
<p><img src="bayesian_files/figure-html/cell-15-output-1.png" width="561" height="182"></p>
</div>
</div>
<p>Now we can just multiply the initial probability density for our uniformed <code>p</code> by either HEADS or TAILS as it comes up in our sequence. As a reminder our sequence was:</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># heads is 1</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0 1 0 0 0 1 0 0 0 0]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>n_trials <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">10</span>]</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">9</span>))</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>post <span class="op">=</span> dist.pdf(x, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>sx <span class="op">=</span> plt.subplot(<span class="bu">len</span>(n_trials)<span class="op">//</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>plt.plot(x, post, label<span class="op">=</span><span class="st">'Uninformed prior'</span>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>plt.setp(sx.get_yticklabels(), visible<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>plt.fill_between(x, <span class="dv">0</span>, post, color<span class="op">=</span><span class="st">"#348ABD"</span>, alpha<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>plt.vlines(<span class="fl">0.5</span>, <span class="dv">0</span>, <span class="dv">2</span>, color<span class="op">=</span><span class="st">"k"</span>, linestyles<span class="op">=</span><span class="st">"--"</span>, lw<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>leg <span class="op">=</span> plt.legend()</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>leg.get_frame().set_alpha(<span class="fl">0.4</span>)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>plt.autoscale(tight<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$p$, probability of heads"</span>) <span class="op">\</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, N <span class="kw">in</span> <span class="bu">enumerate</span>(n_trials):</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>  sx <span class="op">=</span> plt.subplot(<span class="bu">len</span>(n_trials)<span class="op">//</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, <span class="dv">2</span>, k<span class="op">+</span><span class="dv">2</span>)</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>  plt.xlabel(<span class="st">"$p$, probability of heads"</span>) <span class="op">\</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>      <span class="co"># if k in [0, len(n_trials)-1] else None</span></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>  plt.setp(sx.get_yticklabels(), visible<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> data[N<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>    post <span class="op">*=</span> H</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>    post <span class="op">*=</span> T</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>  <span class="co"># y = dist.pdf(x, 1 + heads, 1 + N - heads)</span></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>  plt.plot(x, post, label<span class="op">=</span><span class="st">"observe </span><span class="sc">%d</span><span class="st"> tosses,</span><span class="ch">\n</span><span class="st"> </span><span class="sc">%d</span><span class="st"> heads"</span> <span class="op">%</span> (N, data[:N].<span class="bu">sum</span>()))</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>  plt.fill_between(x, <span class="dv">0</span>, post, color<span class="op">=</span><span class="st">"#348ABD"</span>, alpha<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>  plt.axvline(<span class="fl">0.5</span>, color<span class="op">=</span><span class="st">"k"</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, lw<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>  leg <span class="op">=</span> plt.legend()</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>  leg.get_frame().set_alpha(<span class="fl">0.4</span>)</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>  plt.autoscale(tight<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="bayesian_files/figure-html/cell-17-output-1.png" width="662" height="852"></p>
</div>
</div>
<p>We now have our estimate for <code>p</code> based on 2 heads out of 10 coin tosses. The value of <code>p</code> with the highest likelihood is <code>0.2</code> but there is a non-zero probability for values near zero to all the way past <code>0.6</code>. But what would happen if we toss that same coin more and more and collected more data? We would be able to better refine our estimate for <code>p</code>.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>n_trials <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">400</span>]</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">9</span>))</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.RandomState(<span class="dv">13</span>)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> rng.binomial(n<span class="op">=</span><span class="dv">1</span>, size<span class="op">=</span>n_trials[<span class="op">-</span><span class="dv">1</span>], p<span class="op">=</span>config.p2)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, N <span class="kw">in</span> <span class="bu">enumerate</span>(n_trials):</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    sx <span class="op">=</span> plt.subplot(<span class="bu">len</span>(n_trials)<span class="op">//</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, <span class="dv">2</span>, k<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"$p$, probability of heads"</span>) <span class="op">\</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> k <span class="kw">in</span> [<span class="dv">0</span>, <span class="bu">len</span>(n_trials)<span class="op">-</span><span class="dv">1</span>] <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    plt.setp(sx.get_yticklabels(), visible<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    heads <span class="op">=</span> data[:N].<span class="bu">sum</span>()</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> dist.pdf(x, <span class="dv">1</span> <span class="op">+</span> heads, <span class="dv">1</span> <span class="op">+</span> N <span class="op">-</span> heads)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>    plt.plot(x, y, label<span class="op">=</span><span class="st">"observe </span><span class="sc">%d</span><span class="st"> tosses,</span><span class="ch">\n</span><span class="st"> </span><span class="sc">%d</span><span class="st"> heads"</span> <span class="op">%</span> (N, heads))</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>    plt.fill_between(x, <span class="dv">0</span>, y, color<span class="op">=</span><span class="st">"#348ABD"</span>, alpha<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>    plt.vlines(<span class="fl">0.5</span>, <span class="dv">0</span>, <span class="dv">4</span>, color<span class="op">=</span><span class="st">"k"</span>, linestyles<span class="op">=</span><span class="st">"--"</span>, lw<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>    leg <span class="op">=</span> plt.legend()</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>    leg.get_frame().set_alpha(<span class="fl">0.4</span>)</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>    plt.autoscale(tight<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="bayesian_files/figure-html/cell-18-output-1.png" width="662" height="748"></p>
</div>
</div>
<p>As you can see as we continue to toss the coin past 10 coin tosses, we can further refine our estimate for <code>p</code> such that the value with the highest probability is now <code>0.6</code> whereas when we only had 10 coin tosses, <code>p=0.6</code> has a very low (but non-zero) probability. Note that I’ve using <code>p=0.6</code> throughout this example.</p>
</section>
<section id="where-bayesian-inference-shines" class="level1">
<h1>Where Bayesian Inference Shines</h1>
<p>I find that I turn to bayesian inference in two main situations:</p>
<ol type="1">
<li><strong>When I don’t have a lot data.</strong> It allows you to preserve the highly uncertain nature of your parameters and not imply certainty where none exists.</li>
<li><strong>When I need a data generative model.</strong> Related the first point, there are times when I need to run simulations and experiments and because the outcome of Bayesian inference is a postier distribution for my observed data, I can use this to run experiments. In my next example I will show you this.</li>
</ol>
<section id="practical-example-1-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="practical-example-1-linear-regression">Practical Example 1: Linear Regression</h2>
<p>First off, let’s try linear regression using bayesian inference. Here, I’ve pregenerated a dummy dataset. Let’s import it and plot it.</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> linearregression <span class="im">as</span> lr</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>plt.scatter(lr.x, lr.y)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="bayesian_files/figure-html/cell-19-output-1.png" width="577" height="411"></p>
</div>
</div>
<p>We can now also turn to more sophisticated tools for linear regression. One that I like is PYMC but there are many other packages such as Numpyro, Tensorflow Probability, and Pyro. I’m going to use PYMC here.</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pymc <span class="im">as</span> pm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next we have to build a model for what we think our data generative process might be. A common model for linear regression is:</p>
<p><span class="math display">\[
y_i = a + b x_i + \epsilon
\]</span></p>
<p>Assuming that <span class="math inline">\(\epsilon\)</span> is normally distributed with mean 0 and standard deviation <span class="math inline">\(\sigma\)</span>.</p>
<p><span class="math display">\[
y_i = a + b x_i + \mathcal{N}(0, \sigma)
\]</span></p>
<p>Finally we then have:</p>
<p><span class="math display">\[
y_i = \mathcal{N}(a + b x_i, \sigma)
\]</span></p>
<p>We do this because bayesian inference perfers to think in probability distributions. At the same time, we do not have any prior knowledge of <code>a</code> or <code>b</code> so we will use a uniform distribution as our priors with very large bounds. However one thing we do know from the data is that both <code>a</code> and <code>b</code> are above zero.</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pm.Model() <span class="im">as</span> model:</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># define priors</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>  a <span class="op">=</span> pm.Uniform(<span class="st">'a'</span>, lower<span class="op">=</span><span class="dv">0</span>, upper<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>  b <span class="op">=</span> pm.Uniform(<span class="st">'b'</span>, lower<span class="op">=</span><span class="dv">0</span>, upper<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>  sigma <span class="op">=</span> pm.Uniform(<span class="st">'sigma'</span>, lower<span class="op">=</span><span class="dv">0</span>, upper<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>  y <span class="op">=</span> a <span class="op">+</span> b <span class="op">*</span> x</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># define likelihood</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>  likelihood <span class="op">=</span> pm.Normal(<span class="st">'y'</span>, mu<span class="op">=</span>y, sigma<span class="op">=</span>sigma, observed<span class="op">=</span>lr.y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>So now we have distributions in distributions. The model now has to be fitted.</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> model:</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Interence Here!</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># draw 3000 posterior samples using NUTS sampling</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>  idata <span class="op">=</span> pm.sample(<span class="dv">3000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Auto-assigning NUTS sampler...</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Initializing NUTS using jitter+adapt_diag...</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Multiprocess sampling (4 chains in 4 jobs)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>NUTS: [a, b, sigma]</code></pre>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">

    <div>
      <progress value="16000" class="" max="16000" style="width:300px; height:20px; vertical-align: middle;"></progress>
      100.00% [16000/16000 00:01&lt;00:00 Sampling 4 chains, 80 divergences]
    </div>
    
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Sampling 4 chains for 1_000 tune and 3_000 draw iterations (4_000 + 12_000 draws total) took 3 seconds.</code></pre>
</div>
</div>
</section>
<section id="practical-example-2-pipeline-outage-data" class="level2">
<h2 class="anchored" data-anchor-id="practical-example-2-pipeline-outage-data">Practical Example 2: Pipeline Outage Data</h2>
<p>In this example we’ll be working with pipeline outage data. Let’s say that we have two different locations connected by two different pipelines A and B.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<p>
</p><pre class="mermaid mermaid-js" data-tooltip-selector="#mermaid-tooltip-1">graph LR
    A(Location 1) -- Pipeline A --&gt; B(Location 2)
    A -- Pipeline B --&gt; B
</pre>
<div id="mermaid-tooltip-1" class="mermaidTooltip">

</div>
<p></p>
</div>
</div>
</div>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>