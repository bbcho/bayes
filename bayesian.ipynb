{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Bayesian Inference\n",
        "execute:\n",
        "  echo: true\n",
        "---"
      ],
      "id": "031e185e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pymc as pm"
      ],
      "id": "c6721c37",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can add options to executable code like this\n",
        "\n",
        "\n",
        "```{r}\n",
        "#| echo: false\n",
        "2 * 2\n",
        "```\n",
        "\n",
        "\n",
        "The `echo: false` option disables the printing of code (only output is displayed).\n",
        "\n",
        "## Statistical Inference\n",
        "\n",
        "Statistical inference is the process in which we make inferences (estimates) about a population or problem space when we only have a sub-sample of the total probable outcomes. In the vast majority of cases, we can never collect data for the entire population of the problem space but instead can only sample a subset.\n",
        "\n",
        "For example, some classic problems would be estimating the average height of everyone on earth when we only have the heights of 200 people or determining what is the probability of loan defaults for all potential loan customers given that I have 200 customers and 2 have defaulted in the last year. Another example closer to home would be using the daily returns of a stock or futures contract form the last 2 years (our sample) to estimate the population of ALL possible daily returns - most likely used to model the returns of the stock on contract going forward in time.\n",
        "\n",
        "There are broadly two main approaches to making estimates about a population given data from a sub-sample from the population, a frequentist approach and a Bayesian approach.\n",
        "\n",
        "## Frequestist Approach\n",
        "\n",
        "First lets define frequentist statistics. A frequentist approach to statistics is concerned with calculating the long-run probability of an outcome or event. This is the method that you were taught in school involving a lot of math, probabilities and p-values. The implicit assumption that we're making with this approach is that we have some ability to determine the long-run probability of an outcome. For example the long-run probability of getting heads on a coin toss would be 0.5 based on what we know about the mechanics of a coin toss. However, what if thought that the coin might not be a \"fair\" coin? How would we arrive at the probability of heads then if we had data on tosses of that coin?\n",
        "\n",
        "A frequentist approach to solving this problem (also called frequentist inference) would be to create a hypothesis and test it.\n",
        "\n",
        "https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjRpbe58qH3AhWCM30KHaWkDREQFnoECBsQAw&url=https%3A%2F%2Fvault.hanover.edu%2F~altermattw%2Fcourses%2F220%2Freadings%2FStatistical_Inference.pdf&usg=AOvVaw1htPlxgll-9LxipNreRcQQ\n",
        "\n",
        "First we're going to generate a sequence of 100 coin tosses using an unfair coin.\n"
      ],
      "id": "1945f182"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "p = 0.6 # probability of heads on an unfair coin - this is unknown\n",
        "ntoss = 100\n",
        "\n",
        "rng = np.random.RandomState(13) #15 for 40\n",
        "data = rng.binomial(n=1, size=ntoss, p=p)\n",
        "heads = data.sum()\n",
        "print(\"Number of heads in sample: \", heads)"
      ],
      "id": "edcd1e09",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we have to create a null hypothesis which in our case is that the coin is a fair coin toss. The alternative hypothesis is that the probablity of heads is greater than 50% and thus unfair.\n",
        "\n",
        "h0: p = 0.5 # the null hypothesis that the coin is fair...\n",
        "\n",
        "h1: p > 0.5 # the alternative hypothesis \n",
        "\n",
        "The goal here is to accept or reject the null hypothesis by determining the odds of getting your result under the null hypothesis that the coin is fair.\n",
        "\n",
        "Frequentist inference asks the question, what would be the likelihood of getting 61 heads on 100 coin tosses if the coin was fair? To do this, we'll do the 100 tosses of the fair coin, 10,000 times to see how often we get 61 or more heads out of a hundred.\n"
      ],
      "id": "fbbcb453"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "nsamples = 10000\n",
        "\n",
        "print(f\"Flip {ntoss} coins, {nsamples} times...\\n\")\n",
        "\n",
        "pop = rng.binomial(n=1, size=ntoss*nsamples, p=0.5)\n",
        "sample = rng.choice(pop, size=(ntoss,nsamples))\n",
        "\n",
        "cnt = sample.sum(axis=0)\n",
        "\n",
        "print(\"Number of heads for each sample of 100 tosses (first 20 samples):\\n\")\n",
        "print(cnt[:20])"
      ],
      "id": "b8e3fd45",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's determine how many times we got 61 heads or more...\n"
      ],
      "id": "c0bf47ed"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"probability of getting {heads} heads or more: \", round(len(cnt[cnt >= heads])/nsamples*100,4) , \"%\")"
      ],
      "id": "822a68a6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, because the odds of getting 61 heads or more on a fair coin is only 1.83%, I would reject the null hypothesis h0 that the coin is fair. This is your p-value, the odds of getting this result under the null hypothesis. Note that most statisticians reject the null hypothesis if the p-value is less than 5%. We also call the p-value you false-positive rate - if the coin was fair it would only show 61 heads (and that the coin was unfair) 1.83% of the time, causing you to reject the null hypothesis.\n",
        "\n",
        "However, there are a number of issues with using this approach.\n",
        "\n",
        "## Issue #1: Just because the result is unlikely, doesn't make it impossible\n",
        "\n",
        "Remember, even with the fair coin, we still got 61 or more heads 1.83% of the time. Also, even an unfair coin can sometimes produce results that would be indicative of a fair coin... \n"
      ],
      "id": "91bbd4ae"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rng = np.random.RandomState(12) #15 for 40\n",
        "data = rng.binomial(n=1, size=ntoss, p=p)\n",
        "heads = data.sum()\n",
        "print(\"Number of heads in sample: \", heads)\n",
        "\n",
        "print(f\"probability of getting {heads} heads or more: \", round(len(cnt[cnt >= heads])/nsamples*100,4) , \"%\")"
      ],
      "id": "4d75e72b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Even though we generated this sample using the same probability of heads of 0.6 as before, now we can't reject the null hypothesis!\n",
        "\n",
        "## Issue #2: Picking the right hypothesises is hard\n",
        "\n",
        "In the example above, just because we've rejected the null hypothesis, it doesn't actually tell us what the true value of `p` is! For example another alternative hypothesis could be `p < 0.5`. While this is a simple example with only 3 possible outcomes for p, there are many real life examples where there are many possible explanations aside from the null hypothesis.\n",
        "\n",
        "h0: p = 0.5\n",
        "\n",
        "h1: p > 0.5\n",
        "\n",
        "h2: p < 0.5\n"
      ],
      "id": "b1ee8f82"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "p = 0.4 # this is unknown\n",
        "\n",
        "rng = np.random.RandomState(101) #15 for 40\n",
        "data = rng.binomial(n=1, size=ntoss, p=p)\n",
        "heads = data.sum()\n",
        "print(\"Number of heads in sample: \", heads)\n",
        "\n",
        "print(f\"probability of getting {heads} heads or more: \", round(len(cnt[cnt >= heads])/nsamples*100,4) , \"%\")\n",
        "\n",
        "plt.hist(sample.mean(axis=0))\n",
        "print(\n",
        "    \"2.5th Percentile = \" + str(np.percentile(sample.sum(axis=0), 2.5)) + \" heads\", \n",
        "    \"97.5th Percentile = \" + str(np.percentile(sample.sum(axis=0), 97.5)) + \" heads\"\n",
        "    )"
      ],
      "id": "b5a8b75b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, without knowing the answer which alternative hypothesis would you use? Also it's bad practice just keep on testing various hypothesizes. This is because even with a result that shows a p-value of < 5%, this still means that you could randomly get a false-positive. The chance of getting a false-positive with a p-value of 5% on 20 tests is 100%! i.e. random chance alone will give you a false-positive if you just run enough tests...\n"
      ],
      "id": "edbfe1a6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "p = 0.5\n",
        "rng = np.random.RandomState(10)\n",
        "\n",
        "\n",
        "for i in range(0,20):\n",
        "    data = rng.binomial(n=1, size=ntoss, p=p)\n",
        "    heads = data.sum()\n",
        "\n",
        "    print(f\"probability of getting {heads} heads or more: \", round(len(cnt[cnt >= heads])/nsamples*100,4) , \"%\")"
      ],
      "id": "3da2c435",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Delete this section\n"
      ],
      "id": "9e0334b6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"probability of getting {heads} heads or more: \", round(len(cnt[cnt >= heads])/nsamples*100,4) , \"%\")\n",
        "print(f\"probability of getting {heads} heads or less: \", round(len(cnt[cnt <= heads])/nsamples*100,4) , \"%\")"
      ],
      "id": "9098a07e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"probability of getting 5 heads or more: \", round(len(cnt[cnt >= 5])/nsamples*100,2) , \"%\")"
      ],
      "id": "ba9ec050",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ntoss = 100\n",
        "nsamples = 1000"
      ],
      "id": "0955c897",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data = rng.binomial(n=1, size=ntoss, p=p)\n",
        "heads = data.sum()\n",
        "print(\"Number of heads in sample: \", heads)"
      ],
      "id": "85d29db3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sample = np.random.choice(pop, size=(ntoss,nsamples))"
      ],
      "id": "f6ade292",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cnt = sample.sum(axis=0)\n",
        "cnt[:200]"
      ],
      "id": "cb19f125",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"probability of getting {heads} heads or more: \", round(len(cnt[cnt >= heads])/nsamples*100,2) , \"%\")"
      ],
      "id": "d8cca094",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import scipy"
      ],
      "id": "d1fdde23",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "scipy.stats.binom.cdf(k=1, n=10, p=0.5)"
      ],
      "id": "0c1c0da7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.hist(cnt)"
      ],
      "id": "04474206",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trying to Determine the Long-run Probability Does Not Always Make Sense\n",
        "\n",
        "In many classes of problems, the concept of a long-run probability doesn't always make sense. For example what is the long-run probability that our Sun will go nova tomorrow? How knows? We only have one sun and it's never gone nova before!\n",
        "\n",
        "## Issues\n",
        "\n",
        "So in summary, some issues with using a frequentist approach is that:\n",
        "\n",
        "1. It does not tell you what the most likely answer is...\n",
        "1. It does not tell you if p=0.5 is wrong, just unlikely. You can still get 9 heads with p=0.5. This is your false positive rate.\n",
        "1. Long-run probabilities don't always exist\n",
        "\n",
        "Note that what we've done above is not the true way to calculate the p-value as most statistical methods use formulas to calculate it vs simulations.\n",
        "\n",
        "That's not to say a frequentist approach is bad and in fact bayesian inference converges to the frequentist approach with sufficiently large samples. However let's now have a look at Bayesian Inference.\n",
        "\n",
        "## Bayesian Approach\n",
        "\n",
        "![https://xkcd.com/1132/](https://imgs.xkcd.com/comics/frequentists_vs_bayesians_2x.png)\n",
        "\n",
        "\n",
        "https://www.sciencedirect.com/topics/neuroscience/statistical-inference\n",
        "\n",
        "https://en.wikipedia.org/wiki/Frequentist_inference\n",
        "\n",
        "https://www.redjournal.org/article/S0360-3016(21)03256-9/fulltext\n",
        "\n",
        "https://corporatefinanceinstitute.com/resources/knowledge/other/hypothesis-testing/\n",
        "\n",
        "http://sellsidehandbook.com/2018/12/09/statistical-inference-and-hypothesis-testing/\n",
        "\n",
        "https://www.statisticshowto.com/frequentist-statistics/\n",
        "\n",
        "\n",
        "Bayesian inference and Bayesian statistics in general is named after the statistician Thomas Bayes. \n",
        "\n",
        "In contrast to the frequentist method, Bayesian inference is focused on the probability that something is true. It begins with a measure of belief in a particular model or number, then with the addition of data, this belief is updated to reflect this new data. In my opinion, it is a more intuitive and natural method for incorporating the scientific method into the analytical process.\n",
        "\n",
        "Let's start with the same example as in the frequentist example is an unfair coin (that we don't know).\n",
        "\n",
        "\n",
        "With the first toss we would have no data and so we would have what we call an uniformed prior. Thus, any value for `p` is equally likely.\n"
      ],
      "id": "edcd1f1b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "dist = stats.beta\n",
        "\n",
        "p = 0.6 # probability of heads on an unfair coin - this is unknown\n",
        "ntoss = 100\n",
        "\n",
        "rng = np.random.RandomState(13) #15 for 40\n",
        "data = rng.binomial(n=1, size=ntoss, p=p)\n",
        "heads = data.sum()\n",
        "\n",
        "n_trials = [0, 1, 2, 3, 10, 20, 50, 100]\n",
        "x = np.linspace(0, 1, 100)\n",
        "\n",
        "\n",
        "# For the already prepared, I'm using Binomial's conj. prior.\n",
        "for k, N in enumerate(n_trials):\n",
        "    sx = plt.subplot(len(n_trials)//2, 2, k+1)\n",
        "    plt.xlabel(\"$p$, probability of heads\") \\\n",
        "        if k in [0, len(n_trials)-1] else None\n",
        "    plt.setp(sx.get_yticklabels(), visible=False)\n",
        "    heads = data[:N].sum()\n",
        "    y = dist.pdf(x, 1 + heads, 1 + N - heads)\n",
        "    plt.plot(x, y, label=\"observe %d tosses,\\n %d heads\" % (N, heads))\n",
        "    plt.fill_between(x, 0, y, color=\"#348ABD\", alpha=0.4)\n",
        "    plt.vlines(0.5, 0, 4, color=\"k\", linestyles=\"--\", lw=1)\n",
        "\n",
        "    leg = plt.legend()\n",
        "    leg.get_frame().set_alpha(0.4)\n",
        "    plt.autoscale(tight=True)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "id": "bca25019",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "p = 0.6 # probability of heads on an unfair coin - this is unknown\n",
        "ntoss = 100\n",
        "\n",
        "rng = np.random.RandomState(13) #15 for 40\n",
        "data = rng.binomial(n=1, size=ntoss, p=p)\n",
        "heads = data.sum()\n",
        "print(\"Number of heads in sample: \", heads)\n",
        "print(\"First 10 coin tosses: \", data[:10], \"...\")"
      ],
      "id": "607368fd",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}